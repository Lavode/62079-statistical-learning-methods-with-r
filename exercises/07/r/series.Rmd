---
title: "Statistical Learning Methods with R, Series 7"
author: "Michael Senn"
date: "01/05/2022"
output:
  pdf_document: default
  html_document: default
---    

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```            

Load required libraries

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(FNN)
library(corrplot)
```

---

## Utility functions

As usual, some utility functions.

```{r}
normalize = function(x) {
  (x - min(x)) / (max(x) - min(x))
}
```

```{r}
# Perform x-fold cross validation on a linear model over a given data frame
linear_cv = function(df, formula, x, seed=NA) {
  if (!is.na(seed)) {
    set.seed(seed)
  }
  
  # x-fold cross validation
  n = nrow(df)
  chunkSize = floor(n / x)
  mses = c()
  
  idxRange = 1:n
  permutation = sample(idxRange, n) # Permutation of 1..n
  
  startIdx = 1
  for (i in 1:x) {
    # And these +-1s are why sane languages use semi-open intervals for indexing of collections
    stopIdx = startIdx + chunkSize - 1
    
    # Indices of train/test sets for current fold
    test = permutation[startIdx : stopIdx]
    train = idxRange[-test]
    
    df.train = df[train,]
    df.test = df[test,]
    
    # Train model using train data & use it to predict on test data
    df.lm = lm(formula, data = df.train)
    df.predict = cbind(df.test, predict.lm(df.lm, newdata = df.test, interval = 'confidence'))
    mse = mean((df.predict$mpg - df.predict$fit)^2)
    
    # print(sprintf("Fold %d: MSE = %e", i, mse))
    
    mses = append(mses, mse)
    
    # Start index for next iteration
    startIdx = stopIdx + 1
  }
  
  meanMSE = mean(mses)
  # print(sprintf("Linear regression with %d-fold CV: MSE = %e", x, meanMSE))
  return(list("mean" = meanMSE, "mse" = mses))
}
```



## Comparing linear regression models

We first start by loading the cars data set, cleaning and normalizing it, and checking out the correlation matrix.

```{r}
cars = read.csv("../data/Cars.txt", sep = "\t")
cars = cars %>%
  filter(!is.na(horsepower))

# Why is it interpreted as a char by default?
cars$mpg = as.double(cars$mpg)

# We exclude the name, as that one is bound to be useless
cars = cars[, -9]

summary(cars)

cars.norm = normalize(cars)

cars.cor = cor(
  subset(
    cars.norm[sapply(cars.norm, is.numeric)],
  ),  
  use = "complete.obs",
)
corrplot(cars.cor)
```

We then define three linear regression models to predict the MPG attribute. One purely based on weight, one on all attributes, and one on those which highly correlate with MPG.
```{r}
cars.formula_single = as.formula(mpg ~ weight)
cars.formula_all = as.formula(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin)
cars.formula_multi = as.formula(mpg ~ cylinders + displacement + horsepower + weight)
```

### Evaluation using 10-fold CV

We then compare the three models using ten-fold cross-validation.

```{r}
# We inject a fixed seed into all three runs, to ensure the splits of the CV are equal for all 
# three so that the respective MSEs of each run were using the same split, allowing a more 
# meaningful comparison with the t test.
seed = 42

cars.single.results = linear_cv(cars.norm, cars.formula_single, 10, seed)
cars.all.results = linear_cv(cars.norm, cars.formula_all, 10, seed)
cars.multi.results = linear_cv(cars.norm, cars.formula_multi, 10, seed)

print(sprintf("Single MSE = %e", cars.single.results$mean))
print(sprintf("All MSE = %e", cars.all.results$mean))
print(sprintf("Multi MSE = %e", cars.multi.results$mean))
```

Turns out that, while going from a single linear regression to a multiple linear regression model improves the MSE, running it on all attributes - at least in this data set - provides by far the best result. This is not fully unexpected, as the other attributes such as year and acceleration also have a fairly high correlation with MPG.

### Evaluation using t-test

We can also utilize a t-test on the vector of MSEs of the ten-fold cross-validation, for a more rigorous reasoning.

We first compare the single with the multiple linear regression. While the p value is not particularly high, it is also by far not low enough for us to assume that the two model perform different in a meaningful way.
```{r}
t.test(cars.single.results$mse, cars.multi.results$mse, paired=TRUE, alternative="two.sided")
```


Comparing the single and multiple linear regression with the multiple linear regression using all variables, however, does indeed confirm our assumption that the later model has a significantly different performance from the first two, as the respective p values are very low.
```{r}
t.test(cars.single.results$mse, cars.all.results$mse, paired=TRUE, alternative="two.sided")
t.test(cars.multi.results$mse, cars.all.results$mse, paired=TRUE, alternative="two.sided")
```
For completness' sake we can also run a t-test using the "greater" alternative hypothesis, which indeed confirms that the MSE of the full model is significantly lower than the one of the two others.
```{r}
t.test(cars.single.results$mse, cars.all.results$mse, paired=TRUE, alternative="greater")
t.test(cars.multi.results$mse, cars.all.results$mse, paired=TRUE, alternative="greater")
```
## Logistic regression on cancer dataset

Load the dataset, and turn the diagnosis into a numerical value. 1 For malignant, 0 for benign.

```{r}
cancer = read.csv("../data/Cancer.txt", sep = "\t") %>%
  mutate(Diagnostic = if_else(Diagnostic == "M", 1, 0))

summary(cancer)
```

Split it into train and test, and run a logistical regression on the train dataset.
```{r}
n = nrow(cancer)
idxRange = 1:n
permutation = sample(idxRange, n*0.8) # Permutation of 1..n

cancer.train = cancer[permutation,]
cancer.test = cancer[-permutation,]

cancer.glm = glm(Diagnostic ~ ., data = cancer.train, family = binomial(link = "logit"))
summary(cancer.glm)
```

Turns out it does not converge, but the AIC value is low, so it's probably ok? We'll find out. To do so, we'll check out accuracy, precision, and recall.

```{r}
cancer.predict = predict.glm(cancer.glm, newdata = cancer.test, type = "response")
# We'll use 0.5 as a binarization cutoff
cancer.predict = as.numeric(cancer.predict > 0.5)

correct.idx = cancer.test$Diagnostic == cancer.predict
correct.count = sum(correct.idx)

accuracy = correct.count / nrow(cancer.test)

# Confusion matrix. Mostly it confuses me about what's where
conf = table(cancer.test$Diagnostic, cancer.predict)

actual_positives = sum(cancer.test$Diagnostic)
actual_negatives = nrow(cancer.test) - actual_positives

true_positives = conf[4]
false_negatives = conf[2]
true_positives + false_negatives == actual_positives

true_negatives = conf[1]
false_positives = conf[3]
true_negatives + false_positives == actual_negatives

precision = true_positives / (true_positives + false_positives)
recall = true_positives / actual_positives


print(sprintf("Accuracy = %.2f%%", accuracy*100))
print(sprintf("Recall = %.2f%%", recall*100))
print(sprintf("Precision = %.2f%%", precision*100))
```

Accuracy, precision and recall are all decently high, so the model seems to perform well, and its error is rather low.
